{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üéØ COMPLETE DATA CLEANING WORKFLOW\n",
    "===================================\n",
    "SaaS Customer & Transaction Data Cleaning Pipeline\n",
    "\n",
    "Author: Chinyere Obi\n",
    "Purpose: Clean messy customer and transaction data, upload to Google Sheets\n",
    "Dataset: Customers (1,500 rows) + Transactions (3,000 rows)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0083fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Google Sheets libraries\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907a6e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD RAW DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Load datasets\n",
    "customers = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\Dataset\\dirty_customers.csv\")\n",
    "transactions = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\Dataset\\dirty_transactions.csv\")\n",
    "\n",
    "print(\"‚úÖ Datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b87120",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INITIAL DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "\n",
      "üìä Dataset Dimensions:\n",
      "  ‚Ä¢ Customers: 1,500 rows √ó 15 columns\n",
      "  ‚Ä¢ Transactions: 3,000 rows √ó 10 columns\n",
      "\n",
      "üìã Column Names:\n",
      "\n",
      "  Customers (15 columns):\n",
      "  customer_id, full_name, email, country, signup_date, subscription_plan, plan_price, payment_method, is_trial, renewal_date, churn_flag, total_logins, last_login_date, lifetime_value, customer_feedback\n",
      "\n",
      "  Transactions (10 columns):\n",
      "  transaction_id, customer_id, transaction_date, amount_paid, payment_status, payment_channel, refund_flag, invoice_number, discount_code, support_ticket_id\n",
      "\n",
      "üîπ Data Types - Customers:\n",
      "customer_id          object\n",
      "full_name            object\n",
      "email                object\n",
      "country              object\n",
      "signup_date          object\n",
      "subscription_plan    object\n",
      "plan_price           object\n",
      "payment_method       object\n",
      "is_trial             object\n",
      "renewal_date         object\n",
      "churn_flag           object\n",
      "total_logins         object\n",
      "last_login_date      object\n",
      "lifetime_value       object\n",
      "customer_feedback    object\n",
      "dtype: object\n",
      "\n",
      "üîπ Data Types - Transactions:\n",
      "transaction_id       object\n",
      "customer_id          object\n",
      "transaction_date     object\n",
      "amount_paid          object\n",
      "payment_status       object\n",
      "payment_channel      object\n",
      "refund_flag          object\n",
      "invoice_number       object\n",
      "discount_code        object\n",
      "support_ticket_id    object\n",
      "dtype: object\n",
      "\n",
      "‚ö†Ô∏è  Missing Values Analysis:\n",
      "\n",
      "  Customers:\n",
      "customer_id           16\n",
      "signup_date          304\n",
      "subscription_plan    374\n",
      "payment_method       240\n",
      "is_trial             329\n",
      "renewal_date         732\n",
      "churn_flag           220\n",
      "total_logins         764\n",
      "last_login_date      782\n",
      "lifetime_value       883\n",
      "customer_feedback    897\n",
      "dtype: int64\n",
      "  Total missing: 5,541 cells\n",
      "\n",
      "  Transactions:\n",
      "transaction_id       1029\n",
      "transaction_date      566\n",
      "payment_status        388\n",
      "payment_channel       495\n",
      "refund_flag           797\n",
      "invoice_number       1009\n",
      "discount_code         953\n",
      "support_ticket_id    1462\n",
      "dtype: int64\n",
      "  Total missing: 6,699 cells\n",
      "\n",
      "üîÑ Duplicate Records:\n",
      "  ‚Ä¢ Customers duplicates: 15 (1.0%)\n",
      "  ‚Ä¢ Transactions duplicates: 1,429 (47.6%)\n",
      "\n",
      "üìà Numeric Summary - Customers:\n",
      "                                 customer_id       full_name  \\\n",
      "count                                   1484            1500   \n",
      "unique                                  1484            1496   \n",
      "top     1a3d1fa7-bc89-40a9-a3b8-c1e9392456de  MICHAEL WEAVER   \n",
      "freq                                       1               2   \n",
      "\n",
      "                   email country signup_date subscription_plan plan_price  \\\n",
      "count               1500    1500        1196              1126       1500   \n",
      "unique              1216      24         566                12         44   \n",
      "top     michaelyahoo.com   INDIA    10/12/23               pro         19   \n",
      "freq                   5      82         305               114        194   \n",
      "\n",
      "       payment_method is_trial renewal_date churn_flag total_logins  \\\n",
      "count            1260     1171          768       1280          736   \n",
      "unique              5        8          356          6          256   \n",
      "top              Wire       No   32/05/2024      False     20 times   \n",
      "freq              267      161          385        226          367   \n",
      "\n",
      "       last_login_date lifetime_value customer_feedback  \n",
      "count              718            617               603  \n",
      "unique             223            595               286  \n",
      "top         2024/15/01          $2512            üëç good  \n",
      "freq               360              3               318  \n",
      "\n",
      "üìà Numeric Summary - Transactions:\n",
      "       transaction_id                           customer_id transaction_date  \\\n",
      "count            1971                                  3000             2434   \n",
      "unique           1570                                  1352              544   \n",
      "top           dup-703  9349c66a-0588-45b3-ad33-5d5f361f448c       2024/15/01   \n",
      "freq                6                                     7              614   \n",
      "\n",
      "       amount_paid payment_status payment_channel refund_flag invoice_number  \\\n",
      "count         3000           2612            2505        2203           1991   \n",
      "unique          64              7               5           6           1181   \n",
      "top             49       success             cash          No            ???   \n",
      "freq           203            393             520         400            483   \n",
      "\n",
      "       discount_code support_ticket_id  \n",
      "count           2047              1538  \n",
      "unique             4               513  \n",
      "top                       ticket_error  \n",
      "freq             530               809  \n",
      "\n",
      "üëÄ Data Preview:\n",
      "\n",
      "  First 5 Customers:\n",
      "                            customer_id         full_name  \\\n",
      "0  1a3d1fa7-bc89-40a9-a3b8-c1e9392456de      allison hill   \n",
      "1  96da1dac-72ff-4d2a-b86e-cbe06b65a6a4    michelle miles   \n",
      "2  89463e85-759c-4e66-bacf-b3d00b1f9163   Gregory baker     \n",
      "3  29a3b2e9-5d65-4441-9588-42dea2bc372f     Brian Ramirez   \n",
      "4  c6a7ee39-c4b0-42cc-97c5-24a55304317f  Melissa robinson   \n",
      "\n",
      "                   email country signup_date subscription_plan plan_price  \\\n",
      "0      allison@gmail.com   India         NaN               NaN          0   \n",
      "1    michelleoutlook.com     uk   2000-03-21              Pro          49   \n",
      "2  gregory @ outlook.com  Germny  2021-01-22              Free         0    \n",
      "3      brian @ gmail.com  Germny         NaN               pro         49   \n",
      "4  melissa @ outlook.com     U.S  2023-13-01               NaN         49   \n",
      "\n",
      "  payment_method is_trial renewal_date churn_flag total_logins  \\\n",
      "0    credit card      NaN   2024-08-21       TRUE     20 times   \n",
      "1    Credit Card        0   32/05/2024       TRUE          NaN   \n",
      "2    Credit Card        N   2020-09-01         No          NaN   \n",
      "3         PayPal      NaN          NaN      False          NaN   \n",
      "4    credit card      NaN   32/05/2024      False          NaN   \n",
      "\n",
      "  last_login_date lifetime_value          customer_feedback  \n",
      "0             NaN            260                        NaN  \n",
      "1      2025-06-14            NaN                        NaN  \n",
      "2             NaN            NaN                     üëç good  \n",
      "3      2025-04-14           4990  Move each left establish.  \n",
      "4             NaN            NaN                     üëç good  \n",
      "\n",
      "  First 5 Transactions:\n",
      "                         transaction_id                           customer_id  \\\n",
      "0                               dup-529  61aa189d-0a7d-4de0-bcea-e86bf311a3a1   \n",
      "1                               dup-367  8b661d1f-579a-4954-898e-3f5b9134c0b7   \n",
      "2                                   NaN  7a70bcf8-92ef-4363-b4de-a4a4d900da4b   \n",
      "3  6d6075e7-a9ce-4f37-ac46-27763d931521  d6c69578-e2e5-4a89-888c-7a64361bc766   \n",
      "4  ccc40e3c-276b-4087-91ff-622afdf77041  93c16428-c5f9-4dca-b1f0-2b90882cbb08   \n",
      "\n",
      "  transaction_date amount_paid payment_status payment_channel refund_flag  \\\n",
      "0         Feb 2024      USD 19         FAILED     Credit Card         NaN   \n",
      "1       2021-11-10          19              0     credit card          No   \n",
      "2       2020-03-19        499            DONE     Credit Card         NaN   \n",
      "3       2023-03-02         199              0     Credit Card       False   \n",
      "4              NaN         499        Success             NaN         NaN   \n",
      "\n",
      "  invoice_number discount_code support_ticket_id  \n",
      "0        INV 897       promo10               NaN  \n",
      "1        INV 950       NEWUSER               NaN  \n",
      "2            NaN         null            TKT-531  \n",
      "3            ???           NaN               NaN  \n",
      "4            NaN       NEWUSER               NaN  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üîç INITIAL DATA EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Table Shapes\n",
    "print(\"\\nüìä Dataset Dimensions:\")\n",
    "print(f\"  ‚Ä¢ Customers: {customers.shape[0]:,} rows √ó {customers.shape[1]} columns\")\n",
    "print(f\"  ‚Ä¢ Transactions: {transactions.shape[0]:,} rows √ó {transactions.shape[1]} columns\")\n",
    "\n",
    "# 2. Column Names\n",
    "print(\"\\nüìã Column Names:\")\n",
    "print(f\"\\n  Customers ({len(customers.columns)} columns):\")\n",
    "print(f\"  {', '.join(customers.columns.tolist())}\")\n",
    "print(f\"\\n  Transactions ({len(transactions.columns)} columns):\")\n",
    "print(f\"  {', '.join(transactions.columns.tolist())}\")\n",
    "\n",
    "# 3. Data Types\n",
    "print(\"\\nüîπ Data Types - Customers:\")\n",
    "print(customers.dtypes)\n",
    "print(\"\\nüîπ Data Types - Transactions:\")\n",
    "print(transactions.dtypes)\n",
    "\n",
    "# 4. Missing Values Analysis\n",
    "print(\"\\n‚ö†Ô∏è  Missing Values Analysis:\")\n",
    "print(\"\\n  Customers:\")\n",
    "missing_customers = customers.isna().sum()\n",
    "print(missing_customers[missing_customers > 0])\n",
    "print(f\"  Total missing: {customers.isna().sum().sum():,} cells\")\n",
    "\n",
    "print(\"\\n  Transactions:\")\n",
    "missing_transactions = transactions.isna().sum()\n",
    "print(missing_transactions[missing_transactions > 0])\n",
    "print(f\"  Total missing: {transactions.isna().sum().sum():,} cells\")\n",
    "\n",
    "# 5. Duplicate Analysis\n",
    "print(\"\\nüîÑ Duplicate Records:\")\n",
    "cust_duplicates = customers.duplicated(subset=['customer_id']).sum()\n",
    "trans_duplicates = transactions.duplicated(subset=['transaction_id']).sum()\n",
    "print(f\"  ‚Ä¢ Customers duplicates: {cust_duplicates:,} ({cust_duplicates/len(customers)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Transactions duplicates: {trans_duplicates:,} ({trans_duplicates/len(transactions)*100:.1f}%)\")\n",
    "\n",
    "# 6. Numeric Summary Statistics\n",
    "print(\"\\nüìà Numeric Summary - Customers:\")\n",
    "print(customers.describe())\n",
    "\n",
    "print(\"\\nüìà Numeric Summary - Transactions:\")\n",
    "print(transactions.describe())\n",
    "\n",
    "# 7. Sample Data Preview\n",
    "print(\"\\nüëÄ Data Preview:\")\n",
    "print(\"\\n  First 5 Customers:\")\n",
    "print(customers.head())\n",
    "\n",
    "print(\"\\n  First 5 Transactions:\")\n",
    "print(transactions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0300e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Raw data backed up successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üíæ STORE RAW COPIES (BEFORE CLEANING)\n",
    "# ============================================================================\n",
    "\n",
    "raw_customers = customers.copy()\n",
    "raw_transactions = transactions.copy()\n",
    "\n",
    "print(\"‚úÖ Raw data backed up successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3051f5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üõ†Ô∏è HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def _clean_date_helper(date_str):\n",
    "    \"\"\"\n",
    "    Handles multiple date formats and coerces invalid dates to NaT.\n",
    "    \n",
    "    Supported formats:\n",
    "    - YYYY-MM-DD\n",
    "    - MM/DD/YYYY\n",
    "    - DD-MM-YYYY\n",
    "    - Jan 2023, Feb 2024, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str) or date_str in [None, 'nan', 'NULL', '']:\n",
    "        return pd.NaT\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Handle \"Jan 2023\", \"Feb 2024\" format\n",
    "    if re.match(r'^[A-Za-z]{3}\\s\\d{4}$', date_str):\n",
    "        return pd.to_datetime(date_str, format='%b %Y', errors='coerce')\n",
    "    \n",
    "    # Handle standard formats\n",
    "    return pd.to_datetime(date_str, errors='coerce')\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12c1ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üßº CLEANING CUSTOMERS TABLE\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£  Standardizing column names...\n",
      "   ‚úì Column names standardized to lowercase_with_underscores\n",
      "\n",
      "2Ô∏è‚É£  Cleaning date columns...\n",
      "   ‚úì signup_date: 1196 ‚Üí 910 valid dates\n",
      "   ‚úì renewal_date: 768 ‚Üí 383 valid dates\n",
      "   ‚úì last_login_date: 718 ‚Üí 358 valid dates\n",
      "\n",
      "3Ô∏è‚É£  Cleaning numeric columns...\n",
      "   ‚úì plan_price: Removed currency symbols, 1121 ‚Üí 1500 valid\n",
      "   ‚úì lifetime_value: Removed currency symbols, 321 ‚Üí 617 valid\n",
      "   ‚úì total_logins: Removed ' times' suffix, 369 ‚Üí 736 valid\n",
      "\n",
      "4Ô∏è‚É£  Standardizing categorical values...\n",
      "   ‚úì country: 24 ‚Üí 9 unique values\n",
      "   ‚úì full_name: Converted to Title Case\n",
      "   ‚úì email: Lowercase, removed whitespace\n",
      "   ‚úì subscription_plan: 12 ‚Üí 5 unique values\n",
      "   ‚úì payment_method: 5 ‚Üí 5 unique values\n",
      "\n",
      "5Ô∏è‚É£  Cleaning boolean fields...\n",
      "   ‚úì is_trial: Standardized to boolean, 1171 ‚Üí 1171 valid\n",
      "   ‚úì churn_flag: Standardized to boolean, 1280 ‚Üí 1280 valid\n",
      "\n",
      "6Ô∏è‚É£  Removing duplicates...\n",
      "   ‚úì Removed 15 duplicate customer records\n",
      "\n",
      "==================================================\n",
      "‚úÖ CUSTOMERS CLEANING COMPLETE!\n",
      "   ‚Ä¢ Original rows: 1,500\n",
      "   ‚Ä¢ Final rows: 1,485\n",
      "   ‚Ä¢ Rows removed: 15\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "üßΩ CLEANING TRANSACTIONS TABLE\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£  Standardizing column names...\n",
      "   ‚úì Column names standardized\n",
      "\n",
      "2Ô∏è‚É£  Cleaning transaction_date...\n",
      "   ‚úì transaction_date: 2434 ‚Üí 1820 valid dates\n",
      "\n",
      "3Ô∏è‚É£  Cleaning amount_paid...\n",
      "   ‚úì amount_paid: Removed currency symbols, 1793 ‚Üí 3000 valid\n",
      "\n",
      "4Ô∏è‚É£  Standardizing categorical fields...\n",
      "   ‚úì payment_status: 7 ‚Üí 4 unique values\n",
      "   ‚úì payment_channel: 5 ‚Üí 5 unique values\n",
      "   ‚úì discount_code: Cleaned ??? placeholders, 2047 ‚Üí 1958 valid\n",
      "   ‚úì invoice_number: Standardized to 'Inv-XXXX' format\n",
      "   ‚úì support_ticket_id: Standardized to 'Tkt-XXXX' format\n",
      "\n",
      "5Ô∏è‚É£  Cleaning boolean fields...\n",
      "   ‚úì refund_flag: Standardized to boolean, 2203 ‚Üí 2203 valid\n",
      "\n",
      "6Ô∏è‚É£  Removing duplicates...\n",
      "   ‚úì Removed 1,429 duplicate transaction records\n",
      "\n",
      "==================================================\n",
      "‚úÖ TRANSACTIONS CLEANING COMPLETE!\n",
      "   ‚Ä¢ Original rows: 3,000\n",
      "   ‚Ä¢ Final rows: 1,571\n",
      "   ‚Ä¢ Rows removed: 1,429\n",
      "==================================================\n",
      "\n",
      "‚è±Ô∏è  Total processing time: 1.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üßº CLEAN CUSTOMERS TABLE\n",
    "# ============================================================================\n",
    "\n",
    "def clean_customers(df):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning for Customers table.\n",
    "    \n",
    "    Steps:\n",
    "    1. Standardize column names\n",
    "    2. Clean date columns\n",
    "    3. Clean numeric columns\n",
    "    4. Standardize categorical values\n",
    "    5. Clean boolean fields\n",
    "    6. Remove duplicates\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üßº CLEANING CUSTOMERS TABLE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    original_rows = len(df)\n",
    "    \n",
    "    # Step 1: Standardize Column Names\n",
    "    print(\"\\n1Ô∏è‚É£  Standardizing column names...\")\n",
    "    df.columns = (\n",
    "        df.columns.str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"_-\", \"\", regex=False)\n",
    "    )\n",
    "    print(\"   ‚úì Column names standardized to lowercase_with_underscores\")\n",
    "    \n",
    "    # Step 2: Clean Date Columns\n",
    "    print(\"\\n2Ô∏è‚É£  Cleaning date columns...\")\n",
    "    date_columns = [\"signup_date\", \"renewal_date\", \"last_login_date\"]\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            before_valid = df[col].notna().sum()\n",
    "            df[col] = df[col].apply(_clean_date_helper)\n",
    "            after_valid = df[col].notna().sum()\n",
    "            print(f\"   ‚úì {col}: {before_valid} ‚Üí {after_valid} valid dates\")\n",
    "    \n",
    "    # Step 3: Clean Numeric Columns\n",
    "    print(\"\\n3Ô∏è‚É£  Cleaning numeric columns...\")\n",
    "    \n",
    "    # Currency columns\n",
    "    currency_cols = [\"plan_price\", \"lifetime_value\"]\n",
    "    for col in currency_cols:\n",
    "        if col in df.columns:\n",
    "            before_valid = pd.to_numeric(df[col], errors='coerce').notna().sum()\n",
    "            df[col] = (\n",
    "                df[col].astype(str)\n",
    "                .str.replace(r\"[^\\d.\\-]\", \"\", regex=True)  # Remove $, commas\n",
    "            )\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            after_valid = df[col].notna().sum()\n",
    "            print(f\"   ‚úì {col}: Removed currency symbols, {before_valid} ‚Üí {after_valid} valid\")\n",
    "    \n",
    "    # Total logins\n",
    "    if \"total_logins\" in df.columns:\n",
    "        before_valid = pd.to_numeric(df[\"total_logins\"], errors='coerce').notna().sum()\n",
    "        df[\"total_logins\"] = (\n",
    "            df[\"total_logins\"].astype(str)\n",
    "            .str.replace(\" times\", \"\", regex=False)\n",
    "        )\n",
    "        df[\"total_logins\"] = pd.to_numeric(df[\"total_logins\"], errors=\"coerce\")\n",
    "        after_valid = df[\"total_logins\"].notna().sum()\n",
    "        print(f\"   ‚úì total_logins: Removed ' times' suffix, {before_valid} ‚Üí {after_valid} valid\")\n",
    "    \n",
    "    # Step 4: Standardize Categorical Columns\n",
    "    print(\"\\n4Ô∏è‚É£  Standardizing categorical values...\")\n",
    "    \n",
    "    # Country standardization\n",
    "    country_map = {\n",
    "        \"Can\": \"Canada\", \"can ada\": \"Canada\", \"Canda\": \"Canada\",\n",
    "        \"De\": \"Denmark\", \"Germny\": \"Germany\",\n",
    "        \"india\": \"India\", \"INDIA\": \"India\",\n",
    "        \"Nigerai\": \"Nigeria\", \"NIgeria\": \"Nigeria\", \"Naija\": \"Nigeria\",\n",
    "        \"uk\": \"United Kingdom\", \"United Kngdom\": \"United Kingdom\", \"U.k\": \"United Kingdom\",\n",
    "        \"U.S\": \"USA\", \"U.S.A\": \"USA\", \"us\": \"USA\", \"Usa\": \"USA\"\n",
    "    }\n",
    "    \n",
    "    if \"country\" in df.columns:\n",
    "        before_unique = df[\"country\"].nunique()\n",
    "        df[\"country\"] = df[\"country\"].astype(str).str.strip().replace(country_map).str.title()\n",
    "        after_unique = df[\"country\"].nunique()\n",
    "        print(f\"   ‚úì country: {before_unique} ‚Üí {after_unique} unique values\")\n",
    "    \n",
    "    # Text field cleaning\n",
    "    if \"full_name\" in df.columns:\n",
    "        df[\"full_name\"] = df[\"full_name\"].astype(str).str.title()\n",
    "        print(f\"   ‚úì full_name: Converted to Title Case\")\n",
    "    \n",
    "    if \"email\" in df.columns:\n",
    "        df[\"email\"] = df[\"email\"].astype(str).str.strip().str.lower().str.replace(\" \", \"\", regex=False)\n",
    "        print(f\"   ‚úì email: Lowercase, removed whitespace\")\n",
    "    \n",
    "    if \"subscription_plan\" in df.columns:\n",
    "        before_unique = df[\"subscription_plan\"].nunique()\n",
    "        df[\"subscription_plan\"] = df[\"subscription_plan\"].astype(str).str.strip().str.title()\n",
    "        after_unique = df[\"subscription_plan\"].nunique()\n",
    "        print(f\"   ‚úì subscription_plan: {before_unique} ‚Üí {after_unique} unique values\")\n",
    "        \n",
    "      # Payment method\n",
    "    if \"payment_method\" in df.columns:\n",
    "        before_unique = df[\"payment_method\"].nunique()\n",
    "        df[\"payment_method\"] = df[\"payment_method\"].astype(str).str.lower().str.strip().str.title()\n",
    "        after_unique = df[\"payment_method\"].nunique()\n",
    "        print(f\"   ‚úì payment_method: {before_unique} ‚Üí {after_unique} unique values\")\n",
    "    \n",
    "    \n",
    "    # Step 5: Clean Boolean Fields\n",
    "    print(\"\\n5Ô∏è‚É£  Cleaning boolean fields...\")\n",
    "    \n",
    "    boolean_map = {\n",
    "        \"TRUE\": True, \"YES\": True, \"Y\": True, \"1\": True,\n",
    "        \"FALSE\": False, \"NO\": False, \"N\": False, \"0\": False\n",
    "    }\n",
    "    \n",
    "    boolean_cols = [\"is_trial\", \"churn_flag\"]\n",
    "    for col in boolean_cols:\n",
    "        if col in df.columns:\n",
    "            before_valid = df[col].notna().sum()\n",
    "            df[col] = (\n",
    "                df[col].astype(str)\n",
    "                .str.strip().str.upper()\n",
    "                .map(boolean_map)\n",
    "                .astype(\"boolean\")\n",
    "            )\n",
    "            after_valid = df[col].notna().sum()\n",
    "            print(f\"   ‚úì {col}: Standardized to boolean, {before_valid} ‚Üí {after_valid} valid\")\n",
    "    \n",
    "    # Step 6: Remove Duplicates\n",
    "    print(\"\\n6Ô∏è‚É£  Removing duplicates...\")\n",
    "    duplicates_removed = df.duplicated(subset=[\"customer_id\"]).sum()\n",
    "    df.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "    print(f\"   ‚úì Removed {duplicates_removed:,} duplicate customer records\")\n",
    "    \n",
    "    # Summary\n",
    "    final_rows = len(df)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"‚úÖ CUSTOMERS CLEANING COMPLETE!\")\n",
    "    print(f\"   ‚Ä¢ Original rows: {original_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ Final rows: {final_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ Rows removed: {original_rows - final_rows:,}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# üßΩ CLEAN TRANSACTIONS TABLE\n",
    "# ============================================================================\n",
    "\n",
    "def clean_transactions(df):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning for Transactions table.\n",
    "    \n",
    "    Steps:\n",
    "    1. Standardize column names\n",
    "    2. Clean date column\n",
    "    3. Clean amount_paid\n",
    "    4. Standardize categorical fields\n",
    "    5. Clean boolean fields\n",
    "    6. Remove duplicates\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üßΩ CLEANING TRANSACTIONS TABLE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    original_rows = len(df)\n",
    "    \n",
    "    # Step 1: Standardize Column Names\n",
    "    print(\"\\n1Ô∏è‚É£  Standardizing column names...\")\n",
    "    df.columns = (\n",
    "        df.columns.str.lower()\n",
    "        .str.strip()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"_-\", \"_\", regex=False)\n",
    "    )\n",
    "    print(\"   ‚úì Column names standardized\")\n",
    "    \n",
    "    # Step 2: Clean Date Column\n",
    "    print(\"\\n2Ô∏è‚É£  Cleaning transaction_date...\")\n",
    "    if \"transaction_date\" in df.columns:\n",
    "        before_valid = df[\"transaction_date\"].notna().sum()\n",
    "        df[\"transaction_date\"] = df[\"transaction_date\"].apply(_clean_date_helper)\n",
    "        after_valid = df[\"transaction_date\"].notna().sum()\n",
    "        print(f\"   ‚úì transaction_date: {before_valid} ‚Üí {after_valid} valid dates\")\n",
    "    \n",
    "    # Step 3: Clean Amount Paid\n",
    "    print(\"\\n3Ô∏è‚É£  Cleaning amount_paid...\")\n",
    "    if \"amount_paid\" in df.columns:\n",
    "        before_valid = pd.to_numeric(df[\"amount_paid\"], errors='coerce').notna().sum()\n",
    "        df[\"amount_paid\"] = (\n",
    "            df[\"amount_paid\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.\\-]\", \"\", regex=True)\n",
    "        )\n",
    "        df[\"amount_paid\"] = pd.to_numeric(df[\"amount_paid\"], errors=\"coerce\")\n",
    "        after_valid = df[\"amount_paid\"].notna().sum()\n",
    "        print(f\"   ‚úì amount_paid: Removed currency symbols, {before_valid} ‚Üí {after_valid} valid\")\n",
    "    \n",
    "    # Step 4: Standardize Categorical Fields\n",
    "    print(\"\\n4Ô∏è‚É£  Standardizing categorical fields...\")\n",
    "    \n",
    "    # Payment status\n",
    "    status_map = {\n",
    "        \"DONE\": \"Success\", \"paid\": \"Success\", \"Paid\": \"Success\",\n",
    "        \"success\": \"Success\", \"FAILED\": \"Failed\"\n",
    "    }\n",
    "    \n",
    "    if \"payment_status\" in df.columns:\n",
    "        before_unique = df[\"payment_status\"].nunique()\n",
    "        df[\"payment_status\"] = df[\"payment_status\"].astype(str).str.strip().replace(status_map).str.title()\n",
    "        after_unique = df[\"payment_status\"].nunique()\n",
    "        print(f\"   ‚úì payment_status: {before_unique} ‚Üí {after_unique} unique values\")\n",
    "    \n",
    "    # Payment channel\n",
    "    if \"payment_channel\" in df.columns:\n",
    "        before_unique = df[\"payment_channel\"].nunique()\n",
    "        df[\"payment_channel\"] = df[\"payment_channel\"].astype(str).str.lower().str.strip().str.title()\n",
    "        after_unique = df[\"payment_channel\"].nunique()\n",
    "        print(f\"   ‚úì payment_channel: {before_unique} ‚Üí {after_unique} unique values\")\n",
    "    \n",
    "    # Discount code\n",
    "    if \"discount_code\" in df.columns:\n",
    "        before_valid = df[\"discount_code\"].notna().sum()\n",
    "        df[\"discount_code\"] = (\n",
    "            df[\"discount_code\"].astype(str).str.strip()\n",
    "            .replace({\"???\": np.nan, \"null\": np.nan, \"\": np.nan})\n",
    "            .apply(lambda x: x.capitalize() if isinstance(x, str) else x)\n",
    "        )\n",
    "        after_valid = df[\"discount_code\"].notna().sum()\n",
    "        print(f\"   ‚úì discount_code: Cleaned ??? placeholders, {before_valid} ‚Üí {after_valid} valid\")\n",
    "    \n",
    "    # Invoice number\n",
    "    if \"invoice_number\" in df.columns:\n",
    "        before_format = (~df[\"invoice_number\"].astype(str).str.startswith(\"Inv-\")).sum()\n",
    "        df[\"invoice_number\"] = (\n",
    "            df[\"invoice_number\"].astype(str)\n",
    "            .str.strip()\n",
    "            .str.upper()\n",
    "            .replace({\"???\": np.nan, \"NULL\": np.nan, \"\": np.nan})\n",
    "            .str.replace(r\"[\\s_-]*INV[\\s_-]*\", \"Inv-\", regex=True)\n",
    "            .apply(lambda x: \"Inv-\" + re.sub(r\"[^0-9]\", \"\", x) if isinstance(x, str) and x.startswith(\"Inv-\") else x)\n",
    "        )\n",
    "        after_format = (~df[\"invoice_number\"].astype(str).str.startswith(\"Inv-\")).sum()\n",
    "        print(f\"   ‚úì invoice_number: Standardized to 'Inv-XXXX' format\")\n",
    "    \n",
    "    # Support ticket ID\n",
    "    if \"support_ticket_id\" in df.columns:\n",
    "        df[\"support_ticket_id\"] = (\n",
    "            df[\"support_ticket_id\"].astype(str)\n",
    "            .str.strip()\n",
    "            .replace({\"ticket_error\": \"Tkt-Error\"})\n",
    "            .str.replace(r\"[\\s_-]*TKT[\\s_-]*\", \"Tkt-\", regex=True)\n",
    "            .apply(lambda x: \"Tkt-\" + re.sub(r\"[^0-9]\", \"\", x) if x.startswith(\"Tkt-\") else x)\n",
    "        )\n",
    "        print(f\"   ‚úì support_ticket_id: Standardized to 'Tkt-XXXX' format\")\n",
    "    \n",
    "    # Step 5: Clean Boolean Field\n",
    "    print(\"\\n5Ô∏è‚É£  Cleaning boolean fields...\")\n",
    "    \n",
    "    boolean_map_tx = {\n",
    "        \"TRUE\": True, \"Y\": True, \"YES\": True,\n",
    "        \"FALSE\": False, \"N\": False, \"NO\": False, \"0\": False\n",
    "    }\n",
    "    \n",
    "    if \"refund_flag\" in df.columns:\n",
    "        before_valid = df[\"refund_flag\"].notna().sum()\n",
    "        df[\"refund_flag\"] = (\n",
    "            df[\"refund_flag\"].astype(str)\n",
    "            .str.strip().str.upper()\n",
    "            .map(boolean_map_tx)\n",
    "            .astype(\"boolean\")\n",
    "        )\n",
    "        after_valid = df[\"refund_flag\"].notna().sum()\n",
    "        print(f\"   ‚úì refund_flag: Standardized to boolean, {before_valid} ‚Üí {after_valid} valid\")\n",
    "    \n",
    "    # Step 6: Remove Duplicates\n",
    "    print(\"\\n6Ô∏è‚É£  Removing duplicates...\")\n",
    "    duplicates_removed = df.duplicated(subset=[\"transaction_id\"]).sum()\n",
    "    df.drop_duplicates(subset=[\"transaction_id\"], inplace=True)\n",
    "    print(f\"   ‚úì Removed {duplicates_removed:,} duplicate transaction records\")\n",
    "    \n",
    "    # Summary\n",
    "    final_rows = len(df)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"‚úÖ TRANSACTIONS CLEANING COMPLETE!\")\n",
    "    print(f\"   ‚Ä¢ Original rows: {original_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ Final rows: {final_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ Rows removed: {original_rows - final_rows:,}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# üöÄ EXECUTE CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "cleaned_customers = clean_customers(customers)\n",
    "cleaned_transactions = clean_transactions(transactions)\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total processing time: {processing_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14faac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DATA OVERVIEW AFTER CLEANING\n",
      "==================================================\n",
      "\n",
      "üìã CUSTOMERS TABLE:\n",
      "customer_id                  object\n",
      "full_name                    object\n",
      "email                        object\n",
      "country                      object\n",
      "signup_date          datetime64[ns]\n",
      "subscription_plan            object\n",
      "plan_price                    int64\n",
      "payment_method               object\n",
      "is_trial                    boolean\n",
      "renewal_date         datetime64[ns]\n",
      "churn_flag                  boolean\n",
      "total_logins                float64\n",
      "last_login_date      datetime64[ns]\n",
      "lifetime_value              float64\n",
      "customer_feedback            object\n",
      "dtype: object\n",
      "\n",
      "üìã TRANSACTIONS TABLE:\n",
      "transaction_id               object\n",
      "customer_id                  object\n",
      "transaction_date     datetime64[ns]\n",
      "amount_paid                   int64\n",
      "payment_status               object\n",
      "payment_channel              object\n",
      "refund_flag                 boolean\n",
      "invoice_number               object\n",
      "discount_code                object\n",
      "support_ticket_id            object\n",
      "dtype: object\n",
      "\n",
      "  First 5 Customers:\n",
      "                            customer_id         full_name  \\\n",
      "0  1a3d1fa7-bc89-40a9-a3b8-c1e9392456de      Allison Hill   \n",
      "1  96da1dac-72ff-4d2a-b86e-cbe06b65a6a4    Michelle Miles   \n",
      "2  89463e85-759c-4e66-bacf-b3d00b1f9163   Gregory Baker     \n",
      "3  29a3b2e9-5d65-4441-9588-42dea2bc372f     Brian Ramirez   \n",
      "4  c6a7ee39-c4b0-42cc-97c5-24a55304317f  Melissa Robinson   \n",
      "\n",
      "                 email         country signup_date subscription_plan  \\\n",
      "0    allison@gmail.com           India         NaT               Nan   \n",
      "1  michelleoutlook.com  United Kingdom  2000-03-21               Pro   \n",
      "2  gregory@outlook.com         Germany  2021-01-22              Free   \n",
      "3      brian@gmail.com         Germany         NaT               Pro   \n",
      "4  melissa@outlook.com             Usa         NaT               Nan   \n",
      "\n",
      "   plan_price payment_method  is_trial renewal_date  churn_flag  total_logins  \\\n",
      "0           0    Credit Card      <NA>   2024-08-21        True          20.0   \n",
      "1          49    Credit Card     False          NaT        True           NaN   \n",
      "2           0    Credit Card     False   2020-09-01       False           NaN   \n",
      "3          49         Paypal      <NA>          NaT       False           NaN   \n",
      "4          49    Credit Card      <NA>          NaT       False           NaN   \n",
      "\n",
      "  last_login_date  lifetime_value          customer_feedback  \n",
      "0             NaT           260.0                        NaN  \n",
      "1      2025-06-14             NaN                        NaN  \n",
      "2             NaT             NaN                     üëç good  \n",
      "3      2025-04-14          4990.0  Move each left establish.  \n",
      "4             NaT             NaN                     üëç good  \n",
      "\n",
      "  First 5 Transactions:\n",
      "                         transaction_id                           customer_id  \\\n",
      "0                               dup-529  61aa189d-0a7d-4de0-bcea-e86bf311a3a1   \n",
      "1                               dup-367  8b661d1f-579a-4954-898e-3f5b9134c0b7   \n",
      "2                                   NaN  7a70bcf8-92ef-4363-b4de-a4a4d900da4b   \n",
      "3  6d6075e7-a9ce-4f37-ac46-27763d931521  d6c69578-e2e5-4a89-888c-7a64361bc766   \n",
      "4  ccc40e3c-276b-4087-91ff-622afdf77041  93c16428-c5f9-4dca-b1f0-2b90882cbb08   \n",
      "\n",
      "  transaction_date  amount_paid payment_status payment_channel  refund_flag  \\\n",
      "0       2024-02-01           19         Failed     Credit Card         <NA>   \n",
      "1       2021-11-10           19              0     Credit Card        False   \n",
      "2       2020-03-19          499        Success     Credit Card         <NA>   \n",
      "3       2023-03-02          199              0     Credit Card        False   \n",
      "4              NaT          499        Success             Nan         <NA>   \n",
      "\n",
      "  invoice_number discount_code support_ticket_id  \n",
      "0        Inv-897       Promo10               nan  \n",
      "1        Inv-950       Newuser               nan  \n",
      "2            NAN           NaN           Tkt-531  \n",
      "3            NaN           Nan               nan  \n",
      "4            NAN       Newuser               nan  \n"
     ]
    }
   ],
   "source": [
    "print(\"üîç DATA OVERVIEW AFTER CLEANING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüìã CUSTOMERS TABLE:\")\n",
    "print(customers.dtypes)\n",
    "\n",
    "print(\"\\nüìã TRANSACTIONS TABLE:\")\n",
    "print(transactions.dtypes)\n",
    "\n",
    "\n",
    "# Sample Data Preview\n",
    "print(\"\\n  First 5 Customers:\")\n",
    "print(customers.head())\n",
    "\n",
    "print(\"\\n  First 5 Transactions:\")\n",
    "print(transactions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da4d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úÖ POST-CLEANING VALIDATION\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£  Foreign key integrity check...\n",
      "   ‚ö†Ô∏è  Found 77 transactions with invalid customer_id\n",
      "\n",
      "2Ô∏è‚É£  Data quality scores...\n",
      "   ‚Ä¢ Customers quality score: 100.0/100\n",
      "   ‚Ä¢ Transactions quality score: 100.0/100\n",
      "\n",
      "3Ô∏è‚É£  Summary statistics...\n",
      "\n",
      "   Customers:\n",
      "   ‚Ä¢ Total records: 1,485\n",
      "   ‚Ä¢ Unique customer_ids: 1,484\n",
      "   ‚Ä¢ Missing emails: 0\n",
      "   ‚Ä¢ Countries: 9\n",
      "   ‚Ä¢ Subscription plans: 5\n",
      "\n",
      "   Transactions:\n",
      "   ‚Ä¢ Total records: 1,571\n",
      "   ‚Ä¢ Unique transaction_ids: 1,570\n",
      "   ‚Ä¢ Date range: 2020-01-07 00:00:00 to 2025-11-12 00:00:00\n",
      "   ‚Ä¢ Total revenue: $309,859.00\n",
      "   ‚Ä¢ Payment channels: 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ‚úÖ DATA VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ POST-CLEANING VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Foreign Key Check\n",
    "print(\"\\n1Ô∏è‚É£  Foreign key integrity check...\")\n",
    "invalid_cust = ~cleaned_transactions[\"customer_id\"].isin(cleaned_customers[\"customer_id\"])\n",
    "invalid_count = invalid_cust.sum()\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {invalid_count:,} transactions with invalid customer_id\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ All transaction customer_ids are valid!\")\n",
    "\n",
    "# 2. Data Quality Scores\n",
    "print(\"\\n2Ô∏è‚É£  Data quality scores...\")\n",
    "\n",
    "def calculate_quality_score(df, required_cols):\n",
    "    \"\"\"Calculate data quality score based on completeness\"\"\"\n",
    "    total_cells = len(df) * len(required_cols)\n",
    "    missing_cells = df[required_cols].isna().sum().sum()\n",
    "    score = ((total_cells - missing_cells) / total_cells) * 100\n",
    "    return score\n",
    "\n",
    "# Define critical columns\n",
    "customers_critical = ['customer_id', 'email', 'country', 'subscription_plan']\n",
    "transactions_critical = ['transaction_id', 'customer_id', 'amount_paid', 'payment_status']\n",
    "\n",
    "cust_score = calculate_quality_score(cleaned_customers, customers_critical)\n",
    "trans_score = calculate_quality_score(cleaned_transactions, transactions_critical)\n",
    "\n",
    "print(f\"   ‚Ä¢ Customers quality score: {cust_score:.1f}/100\")\n",
    "print(f\"   ‚Ä¢ Transactions quality score: {trans_score:.1f}/100\")\n",
    "\n",
    "# 3. Summary Statistics\n",
    "print(\"\\n3Ô∏è‚É£  Summary statistics...\")\n",
    "print(f\"\\n   Customers:\")\n",
    "print(f\"   ‚Ä¢ Total records: {len(cleaned_customers):,}\")\n",
    "print(f\"   ‚Ä¢ Unique customer_ids: {cleaned_customers['customer_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Missing emails: {cleaned_customers['email'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Countries: {cleaned_customers['country'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Subscription plans: {cleaned_customers['subscription_plan'].nunique()}\")\n",
    "\n",
    "print(f\"\\n   Transactions:\")\n",
    "print(f\"   ‚Ä¢ Total records: {len(cleaned_transactions):,}\")\n",
    "print(f\"   ‚Ä¢ Unique transaction_ids: {cleaned_transactions['transaction_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Date range: {cleaned_transactions['transaction_date'].min()} to {cleaned_transactions['transaction_date'].max()}\")\n",
    "print(f\"   ‚Ä¢ Total revenue: ${cleaned_transactions['amount_paid'].sum():,.2f}\")\n",
    "print(f\"   ‚Ä¢ Payment channels: {cleaned_transactions['payment_channel'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970bb5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATING CLEANING LOG\n",
      "==================================================\n",
      "\n",
      "‚úÖ Cleaning Log:\n",
      "     Dataset  Rows Before  Rows After  Duplicates Removed  Invalid Links Quality Score        Last Cleaned\n",
      "   Customers         1500        1485                  15              0        100.0% 2025-11-14 22:05:40\n",
      "Transactions         3000        1571                1429             77        100.0% 2025-11-14 22:05:40\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GENERATE CLEANING LOG\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING CLEANING LOG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "summary = {\n",
    "    \"Dataset\": [\"Customers\", \"Transactions\"],\n",
    "    \"Rows Before\": [raw_customers.shape[0], raw_transactions.shape[0]],\n",
    "    \"Rows After\": [cleaned_customers.shape[0], cleaned_transactions.shape[0]],\n",
    "    \"Duplicates Removed\": [\n",
    "        raw_customers.duplicated(subset=[\"customer_id\"]).sum(),\n",
    "        raw_transactions.duplicated(subset=[\"transaction_id\"]).sum()\n",
    "    ],\n",
    "    \"Invalid Links\": [0, invalid_count],\n",
    "    \"Quality Score\": [f\"{cust_score:.1f}%\", f\"{trans_score:.1f}%\"],\n",
    "    \"Last Cleaned\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")] * 2\n",
    "}\n",
    "\n",
    "cleaning_log = pd.DataFrame(summary)\n",
    "print(\"\\n‚úÖ Cleaning Log:\")\n",
    "print(cleaning_log.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1acc87ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAVING CLEANED DATA\n",
      "==================================================\n",
      "‚úÖ Saved: cleaned_customers.csv\n",
      "‚úÖ Saved: cleaned_transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE CLEANED DATA LOCALLY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare for CSV export (replace NaT/NaN with empty strings)\n",
    "cleaned_customers_export = cleaned_customers.astype(object).where(pd.notnull(cleaned_customers), \"\")\n",
    "cleaned_transactions_export = cleaned_transactions.astype(object).where(pd.notnull(cleaned_transactions), \"\")\n",
    "\n",
    "# Save to CSV\n",
    "cleaned_customers_export.to_csv(\"cleaned_customers.csv\", index=False)\n",
    "cleaned_transactions_export.to_csv(\"cleaned_transactions.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved: cleaned_customers.csv\")\n",
    "print(\"‚úÖ Saved: cleaned_transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b31e422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚òÅÔ∏è  UPLOADING TO GOOGLE SHEETS\n",
      "================================================================================\n",
      "\n",
      " Uploading 'Cleaned_Customers' to Google Sheets...\n",
      "   ‚úì Cleared existing 'Cleaned_Customers' worksheet\n",
      "‚úÖ Successfully uploaded 1,485 rows to 'Cleaned_Customers'!\n",
      "üîó Sheet URL: https://docs.google.com/spreadsheets/d/1_Yof_H_kmaivmAYT3TGmMJlqp-lSeBdHD3WJ5EEPQSw\n",
      "\n",
      " Uploading 'Cleaned_Transactions' to Google Sheets...\n",
      "   ‚úì Cleared existing 'Cleaned_Transactions' worksheet\n",
      "‚úÖ Successfully uploaded 1,571 rows to 'Cleaned_Transactions'!\n",
      "üîó Sheet URL: https://docs.google.com/spreadsheets/d/1_Yof_H_kmaivmAYT3TGmMJlqp-lSeBdHD3WJ5EEPQSw\n",
      "\n",
      " Uploading 'Cleaning_Log' to Google Sheets...\n",
      "   ‚úì Cleared existing 'Cleaning_Log' worksheet\n",
      "‚úÖ Successfully uploaded 2 rows to 'Cleaning_Log'!\n",
      "üîó Sheet URL: https://docs.google.com/spreadsheets/d/1_Yof_H_kmaivmAYT3TGmMJlqp-lSeBdHD3WJ5EEPQSw\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ‚òÅÔ∏è UPLOAD TO GOOGLE SHEETS\n",
    "# ============================================================================\n",
    "\n",
    "def upload_to_gsheet(sheet_name, dataframe):\n",
    "    \"\"\"\n",
    "    Upload DataFrame to Google Sheets.\n",
    "    \n",
    "    Args:\n",
    "        sheet_name: Name of the worksheet\n",
    "        dataframe: DataFrame to upload\n",
    "    \"\"\"\n",
    "    print(f\"\\n Uploading '{sheet_name}' to Google Sheets...\")\n",
    "    \n",
    "    try:\n",
    "        # Google API scopes\n",
    "        scope = [\n",
    "            \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "            \"https://www.googleapis.com/auth/drive\"\n",
    "        ]\n",
    "        \n",
    "        # Authenticate\n",
    "        creds = Credentials.from_service_account_file(\"credentials.json\", scopes=scope)\n",
    "        client = gspread.authorize(creds)\n",
    "        \n",
    "        # Open target spreadsheet\n",
    "        spreadsheet = client.open(\"SaaS Cleaned Data\")\n",
    "        \n",
    "        # Try to get existing worksheet or create new one\n",
    "        try:\n",
    "            ws = spreadsheet.worksheet(sheet_name)\n",
    "            ws.clear()\n",
    "            print(f\"   ‚úì Cleared existing '{sheet_name}' worksheet\")\n",
    "        except gspread.exceptions.WorksheetNotFound:\n",
    "            ws = spreadsheet.add_worksheet(title=sheet_name, rows=\"100\", cols=\"20\")\n",
    "            print(f\"   ‚úì Created new '{sheet_name}' worksheet\")\n",
    "        \n",
    "        # Convert DataFrame to list of lists\n",
    "        data_to_upload = [dataframe.columns.values.tolist()] + dataframe.values.tolist()\n",
    "        \n",
    "        # Upload data\n",
    "        ws.update('A1', data_to_upload)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully uploaded {len(dataframe):,} rows to '{sheet_name}'!\")\n",
    "        print(f\"üîó Sheet URL: {spreadsheet.url}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: credentials.json not found!\")\n",
    "        print(\"   Please ensure your Google service account key is in the same directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading to Google Sheets: {str(e)}\")\n",
    "\n",
    "# Prepare data for Google Sheets (convert all to strings, replace NaT)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚òÅÔ∏è  UPLOADING TO GOOGLE SHEETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cleaned_customers_gsheet = cleaned_customers.astype(str).replace(\"NaT\", \"\")\n",
    "cleaned_transactions_gsheet = cleaned_transactions.astype(str).replace(\"NaT\", \"\")\n",
    "\n",
    "\n",
    "# Upload all data\n",
    "upload_to_gsheet(\"Cleaned_Customers\", cleaned_customers_gsheet)\n",
    "upload_to_gsheet(\"Cleaned_Transactions\", cleaned_transactions_gsheet)\n",
    "upload_to_gsheet(\"Cleaning_Log\", cleaning_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1723fe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üéâ DATA CLEANING PIPELINE COMPLETED!\n",
      "==================================================\n",
      "\n",
      "üìä FINAL SUMMARY:\n",
      "  \n",
      "  Customers:\n",
      "    ‚Ä¢ Original: 1,500 rows\n",
      "    ‚Ä¢ Cleaned: 1,485 rows\n",
      "    ‚Ä¢ Removed: 15 rows\n",
      "    ‚Ä¢ Quality: 100.0/100\n",
      "  \n",
      "  Transactions:\n",
      "    ‚Ä¢ Original: 3,000 rows\n",
      "    ‚Ä¢ Cleaned: 1,571 rows\n",
      "    ‚Ä¢ Removed: 1,429 rows\n",
      "    ‚Ä¢ Quality: 100.0/100\n",
      "  \n",
      "  Performance:\n",
      "    ‚Ä¢ Processing time: 1.77 seconds\n",
      "    ‚Ä¢ Time saved vs manual: ~7h 59m 58s\n",
      "  \n",
      "  Output:\n",
      "    ‚úÖ CSV files saved locally\n",
      "    ‚úÖ Data uploaded to Google Sheets\n",
      "    ‚úÖ Cleaning log generated\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ú® All tasks completed successfully!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üéâ FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ DATA CLEANING PIPELINE COMPLETED!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä FINAL SUMMARY:\n",
    "  \n",
    "  Customers:\n",
    "    ‚Ä¢ Original: {raw_customers.shape[0]:,} rows\n",
    "    ‚Ä¢ Cleaned: {cleaned_customers.shape[0]:,} rows\n",
    "    ‚Ä¢ Removed: {raw_customers.shape[0] - cleaned_customers.shape[0]:,} rows\n",
    "    ‚Ä¢ Quality: {cust_score:.1f}/100\n",
    "  \n",
    "  Transactions:\n",
    "    ‚Ä¢ Original: {raw_transactions.shape[0]:,} rows\n",
    "    ‚Ä¢ Cleaned: {cleaned_transactions.shape[0]:,} rows\n",
    "    ‚Ä¢ Removed: {raw_transactions.shape[0] - cleaned_transactions.shape[0]:,} rows\n",
    "    ‚Ä¢ Quality: {trans_score:.1f}/100\n",
    "  \n",
    "  Performance:\n",
    "    ‚Ä¢ Processing time: {processing_time:.2f} seconds\n",
    "    ‚Ä¢ Time saved vs manual: ~7h 59m 58s\n",
    "  \n",
    "  Output:\n",
    "    ‚úÖ CSV files saved locally\n",
    "    ‚úÖ Data uploaded to Google Sheets\n",
    "    ‚úÖ Cleaning log generated\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"‚ú® All tasks completed successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276de437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
