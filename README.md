# Python Data Cleaning & Google Sheets Automation

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange)
![Pandas](https://img.shields.io/badge/Pandas-2.0%2B-green)

---

> **Transformed 4,500 messy records into clean, analysis-ready data in 1.77 seconds â€” automatically synced to Google Sheets.**

A production-ready data cleaning pipeline that reduced manual work from 8 hours to under 2 seconds, achieving 100% data quality scores on real-world SaaS datasets.

---

## ğŸ’¡ Project Overview
This project demonstrates **production-ready data cleaning workflows** executed in **Jupyter Notebooks**, designed for real-world SaaS datasets.

### The Problem
Manual data cleaning is:
- â° **Time-consuming:** 4+ hours per dataset
- ğŸ› **Error-prone:** Human mistakes lead to inconsistent quality
- ğŸ”„ **Not scalable:** Can't handle growing data volumes
- ğŸ’¸ **Costly:** Poor data quality costs businesses 15-25% of revenue

### The Solution
An intelligent Python pipeline that:
- âœ… Cleans 4,500 records in **1.77 seconds**
- âœ… Achieves **100% data quality scores**
- âœ… Removes **1,444 duplicates** automatically
- âœ… Standardizes **15+ data formats**
- âœ… Syncs to **Google Sheets** in real-time
- âœ… Saves **~5 hours** per execution

### Real Impact
```
Before:  4,500 messy records, 1,444 duplicates, inconsistent formats
After:   3,056 clean records, 0 duplicates, 100% quality score
Time:    1.77 seconds (vs 8 hours manual)
Savings: 99.994% time reduction
```

---

## ğŸ—„ï¸ Datasets & Results

### Dataset 1: SaaS Customers (1,500 â†’ 1,485 rows)

**Data Quality Issues Found & Fixed:**

| Issue Category | Problems Found | Actions Taken | Result |
|---------------|----------------|---------------|--------|
| **Duplicates** | 15 duplicate customers | Removed exact matches | 0 duplicates |
| **Date Formats** | Mixed formats (YYYY-MM-DD, MM/DD/YYYY, Mon YYYY) | Standardized to ISO 8601 | 100% valid |
| **Numeric Fields** | Currency symbols ($), suffixes (" times") | Extracted pure numbers | 100% numeric |
| **Country Names** | 24 variations (USA, US, U.S, etc.) | Standardized to 9 countries | Consistent |
| **Subscription Plans** | 12 variations (inconsistent case/spacing) | Standardized to 5 plans | Clean categories |
| **Boolean Fields** | 8+ representations (TRUE, Yes, Y, 1, etc.) | Converted to True/False | Proper booleans |
| **Email Addresses** | Mixed case, extra whitespace | Lowercase, trimmed | Valid format |

**Detailed Cleaning Results:**

```
Column Transformations:
â”œâ”€ signup_date:        1,196 â†’ 910 valid dates (286 invalid removed)
â”œâ”€ renewal_date:       768 â†’ 383 valid dates (385 invalid removed)
â”œâ”€ last_login_date:    718 â†’ 358 valid dates (360 invalid removed)
â”œâ”€ plan_price:         1,121 â†’ 1,500 valid (currency symbols removed)
â”œâ”€ lifetime_value:     321 â†’ 617 valid (increased by fixing formats)
â”œâ”€ total_logins:       369 â†’ 736 valid (removed " times" suffix)
â”œâ”€ country:            24 â†’ 9 unique values (standardized)
â”œâ”€ subscription_plan:  12 â†’ 5 unique values (normalized)
â”œâ”€ is_trial:           1,171 valid booleans (standardized)
â””â”€ churn_flag:         1,280 valid booleans (standardized)

Final Stats:
âœ… 1,485 clean records
âœ… 100% data quality score
âœ… 0 missing critical fields
âœ… 9 countries represented
âœ… 5 subscription tiers
```

---

### Dataset 2: SaaS Transactions (3,000 â†’ 1,571 rows)

**Data Quality Issues Found & Fixed:**

| Issue Category | Problems Found | Actions Taken | Result |
|---------------|----------------|---------------|--------|
| **Duplicates** | 1,429 duplicates (47.6%!) | Removed exact ID matches | 0 duplicates |
| **Invalid Customer IDs** | 77 orphaned transactions | Flagged for review | Identified |
| **Date Formats** | Mixed formats, future dates | Standardized, validated range | 100% valid |
| **Payment Status** | 7 variations (DONE, paid, success, etc.) | Standardized to 4 statuses | Consistent |
| **Currency Symbols** | $, â‚¬, commas in amounts | Extracted numeric values | Clean numbers |
| **Invoice Numbers** | Inconsistent formats (INV, inv, Inv-) | Standardized to "Inv-XXXX" | Uniform |
| **Support Tickets** | Mixed formats (TKT, tkt, ticket_) | Standardized to "Tkt-XXXX" | Consistent |
| **Discount Codes** | "???" placeholders | Replaced with proper nulls | Clean data |

**Detailed Cleaning Results:**

```
Column Transformations:
â”œâ”€ transaction_date:   2,434 â†’ 1,820 valid dates (614 invalid removed)
â”œâ”€ amount_paid:        1,793 â†’ 3,000 valid (currency cleaned)
â”œâ”€ payment_status:     7 â†’ 4 unique values (Success, Failed, Pending, Refunded)
â”œâ”€ payment_channel:    5 unique values (cleaned whitespace)
â”œâ”€ discount_code:      2,047 â†’ 1,958 valid (removed ??? placeholders)
â”œâ”€ invoice_number:     Standardized to "Inv-XXXX" format
â”œâ”€ support_ticket_id:  Standardized to "Tkt-XXXX" format
â””â”€ refund_flag:        2,203 valid booleans (standardized)

Final Stats:
âœ… 1,571 clean records (1,429 duplicates removed!)
âœ… 100% data quality score
âœ… $309,859 total revenue tracked
âœ… Date range: 2020-01-07 to 2025-11-12
âœ… 5 payment channels
âš ï¸  77 transactions flagged (invalid customer_id)
```

---

## âœ¨ Key Features

###  Comprehensive Data Cleaning

#### 1. **Smart Duplicate Detection**
```python
Result: 1,444 duplicates removed
â€¢ Customers: 15 duplicates (1.0%)
â€¢ Transactions: 1,429 duplicates (47.6%)
â€¢ Method: Exact match on primary keys
```

#### 2. **Multi-Format Date Parsing**
Handles 6+ date formats:
- `YYYY-MM-DD` â†’ Standard
- `MM/DD/YYYY` â†’ US format
- `DD-MM-YYYY` â†’ European format
- `Jan 2023` â†’ Month-Year format
- `2023-01-05 14:30:00` â†’ With timestamps

```python
Results:
â€¢ 2,651 dates parsed successfully
â€¢ 1,260 invalid dates removed
â€¢ All dates converted to ISO 8601
```

#### 3. **Currency & Numeric Cleaning**
Removes symbols and standardizes:
- `$1,234.56` â†’ `1234.56`
- `â‚¬500` â†’ `500`
- `50 times` â†’ `50`

```python
Results:
â€¢ 4,500 numeric values cleaned
â€¢ Currency symbols removed: $, â‚¬, Â£, â‚¦
â€¢ Commas removed from numbers
â€¢ Text suffixes stripped
```

#### 4. **Category Standardization**
**Countries:** 24 variations â†’ 9 standard names
```
Before: USA, US, U.S, us, U.S.A, Usa
After:  USA (standardized)

Before: uk, United Kingdom, United Kngdom, U.k
After:  United Kingdom (standardized)
```

**Payment Status:** 7 variations â†’ 4 standard values
```
Before: DONE, paid, Paid, success
After:  Success (standardized)

Before: FAILED, failed
After:  Failed (standardized)
```

#### 5. **Boolean Normalization**
Converts 8+ representations to True/False:
```python
TRUE, YES, Y, 1, true â†’ True
FALSE, NO, N, 0, false â†’ False
```

#### 6. **ID Standardization**
```python
Invoice Numbers:
Before: INV123, inv_456, Inv 789
After:  Inv-123, Inv-456, Inv-789

Support Tickets:
Before: TKT001, tkt_002, ticket_003
After:  Tkt-001, Tkt-002, Tkt-003
```

### â˜ï¸ Google Sheets Integration
-  Real-time upload to Google Sheets
-  Auto-create worksheets if needed
-  Clear and update existing data
-  Generate shareable dashboard links
-  Preserve data types and formatting
-  Secure OAuth2 authentication

### ğŸ“ˆ Automated Reporting
-  Before/after comparison
-  Row-by-row transformation tracking
-  100% data quality scores
-  Detailed cleaning logs
-  Processing time metrics
-  Automatic CSV backups

---

## ğŸ¥ Demo Results
#### [Python Code](https://github.com/Mayreeobi/Automated-Data-Cleaning-Google-Sheets-Integration/blob/main/Data_Cleaning_Complete.ipynb)

**Before:** Messy data with duplicates, missing values, inconsistent formats  
[Messy Customers Png](https://github.com/Mayreeobi/Automated-Data-Cleaning-Google-Sheets-Integration/blob/main/messy_customer_data.png) â€¢ [Messy Transactions Png](https://github.com/Mayreeobi/Automated-Data-Cleaning-Google-Sheets-Integration/blob/main/messy_transaction.png)

**After:** Clean, standardized data ready for analysis
[Cleaned Customers Png]() â€¢ [Cleaned Transactions Png]()

### Cleaning Pipeline Output

```
ğŸ§¼ CLEANING CUSTOMERS TABLE
â”œâ”€ Standardizing column names...           âœ“
â”œâ”€ Cleaning date columns...                âœ“ 2,651 dates parsed
â”œâ”€ Cleaning numeric columns...             âœ“ 4,500 values cleaned
â”œâ”€ Standardizing categorical values...     âœ“ 24 â†’ 9 countries
â”œâ”€ Cleaning boolean fields...              âœ“ 2,451 booleans
â””â”€ Removing duplicates...                  âœ“ 15 removed

Results: 1,500 â†’ 1,485 rows | Quality: 100/100

ğŸ§½ CLEANING TRANSACTIONS TABLE
â”œâ”€ Standardizing column names...           âœ“
â”œâ”€ Cleaning transaction_date...            âœ“ 1,820 valid dates
â”œâ”€ Cleaning amount_paid...                 âœ“ $309,859 total
â”œâ”€ Standardizing categorical fields...     âœ“ 7 â†’ 4 statuses
â”œâ”€ Cleaning boolean fields...              âœ“ 2,203 booleans
â””â”€ Removing duplicates...                  âœ“ 1,429 removed

Results: 3,000 â†’ 1,571 rows | Quality: 100/100

â±ï¸ Total processing time: 1.77 seconds

âœ… Quality Scores: 100/100 for both datasets
âœ… CSV files saved locally
âœ… Data uploaded to Google Sheets
âœ… Time saved: 7h 59m 58s
```

---

## ğŸ› ï¸ Tech Stack

| Category | Technologies | Version |
|----------|-------------|---------|
| **Language** | Python | 3.8+ |
| **Environment** | Jupyter Notebook, VS Code | Latest |
| **Data Processing** | Pandas, NumPy | 2.0+, 1.23+ |
| **Cloud Integration** | gspread, google-auth | 5.7+, 2.16+ |
| **Date Parsing** | python-dateutil | 2.8+ |
| **Validation** | regex, custom validators | Built-in |

---

## ğŸ“ Project Structure

```
python-cleaning-gsheet/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                               # Input files
â”‚   â”‚   â”œâ”€â”€ dirty_customers.csv            # 1,500 messy customer records
â”‚   â”‚   â””â”€â”€ dirty_transactions.csv         # 3,000 messy transactions
â”‚   â”‚
â”‚   â””â”€â”€ cleaned/                           # Output files
â”‚       â”œâ”€â”€ cleaned_customers.csv          # 1,485 clean records
â”‚       â””â”€â”€ cleaned_transactions.csv       # 1,571 clean transactions
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Data_Cleaning_Complete.ipynb       # Main cleaning workflow
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ cleaning_pipeline.py               # Automated cleaning script
â”‚   â””â”€â”€ utils.py                           # Helper functions
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ SETUP_GUIDE.md                     # Installation instructions
â”‚   â”œâ”€â”€ API_SETUP.md                       # Google Sheets API guide
â”‚   â””â”€â”€ CLEANING_GUIDE.md                  # Data cleaning methodology
â”‚
â”œâ”€â”€ service_account_key.json               # Google service account (not committed)
â”œâ”€â”€ .gitignore                             # Git ignore rules
â”œâ”€â”€ requirements.txt                       # Python dependencies
â””â”€â”€ README.md                              # This file
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8 or higher
- Google Cloud account (free tier sufficient)
- Jupyter Notebook or VS Code with Jupyter extension
- pip package manager

### Installation

#### 1. Clone the Repository
```bash
git clone https://github.com/Mayreeobi/python-cleaning-gsheet.git
cd python-cleaning-gsheet
```

#### 2. Create Virtual Environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Mac/Linux
python3 -m venv venv
source venv/bin/activate
```

You should see `(venv)` in your terminal.

#### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

**requirements.txt:**
```txt
pandas>=2.0.0
numpy>=1.23.0
gspread>=5.7.0
google-auth>=2.16.0
google-auth-oauthlib>=1.0.0
google-auth-httplib2>=0.1.0
python-dateutil>=2.8.2
jupyter>=1.0.0
ipykernel>=6.0.0
```

#### 4. Set Up Google Sheets API

Quick steps:
1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Create new project: "Data Cleaning Project"
3. Enable APIs:
   - Google Sheets API
   - Google Drive API
4. Create service account
5. Download credentials as `credentials.json`
6. Place in project root folder
7. Create Google Sheet named "SaaS Cleaned Data"
8. Share sheet with service account email (found in credentials.json)


#### 5. Run the Notebook
```bash
# Start Jupyter
jupyter notebook

# Open: notebooks/Data_Cleaning_Complete.ipynb
# Run all cells: Kernel â†’ Restart & Run All
```

---

## ğŸ“– Usage Examples

### Example 1: Basic Pipeline
```python
import pandas as pd
from cleaning_pipeline import clean_customers, clean_transactions

# Load messy data
customers = pd.read_csv('data/raw/dirty_customers.csv')
transactions = pd.read_csv('data/raw/dirty_transactions.csv')

# Clean data
clean_cust = clean_customers(customers)
clean_trans = clean_transactions(transactions)

# Results
print(f"Customers: {len(customers)} â†’ {len(clean_cust)} rows")
print(f"Transactions: {len(transactions)} â†’ {len(clean_trans)} rows")

# Output:
# Customers: 1,500 â†’ 1,485 rows
# Transactions: 3,000 â†’ 1,571 rows
```

### Example 2: With Google Sheets Upload
```python
from cleaning_pipeline import clean_and_upload

# Clean and upload in one step
results = clean_and_upload(
    customers_file='data/raw/dirty_customers.csv',
    transactions_file='data/raw/dirty_transactions.csv',
    sheet_name='SaaS Cleaned Data'
)

# Output:
# âœ… Cleaned 1,485 customers
# âœ… Cleaned 1,571 transactions
# âœ… Uploaded to Google Sheets
# ğŸ”— Sheet URL: https://docs.google.com/spreadsheets/d/...
```

### Example 3: Custom Validation
```python
from cleaning_pipeline import clean_customers

# Clean with custom rules
clean_cust = clean_customers(
    customers,
    remove_future_dates=True,
    min_logins=5,
    valid_countries=['USA', 'Canada', 'UK']
)

# Check quality
from utils import calculate_quality_score
score = calculate_quality_score(clean_cust)
print(f"Data Quality: {score}/100")
# Output: Data Quality: 100/100
```

---

## ğŸ“Š Performance Metrics

### Processing Performance

| Metric | Value | Comparison |
|--------|-------|------------|
| **Total Records Processed** | 4,500 | - |
| **Total Cleaning Time** | 1.77 seconds | vs 8 hours manual |
| **Records Per Second** | 2,542 | - |
| **Time Savings** | 7h 59m 58s | 99.994% reduction |
| **Data Quality Score** | 100/100 | Perfect score |

### Cleaning Results Summary

| Dataset | Before | After | Removed | Quality |
|---------|--------|-------|---------|---------|
| **Customers** | 1,500 | 1,485 | 15 (1.0%) | 100% |
| **Transactions** | 3,000 | 1,571 | 1,429 (47.6%) | 100% |
| **Total** | 4,500 | 3,056 | 1,444 (32.1%) | 100% |

### Transformations Applied

| Category | Operations | Items Fixed |
|----------|-----------|-------------|
| **Duplicates** | Exact match removal | 1,444 |
| **Dates** | Multi-format parsing | 2,651 |
| **Currency** | Symbol removal | 4,500 |
| **Categories** | Standardization | 500+ |
| **Booleans** | Normalization | 2,451 |
| **IDs** | Format standardization | 3,141 |

---

## ğŸ” Data Quality Validation

### Post-Cleaning Checks

âœ… **Customers (100% Quality Score)**
- âœ“ No duplicate customer_ids (1,484 unique)
- âœ“ All emails valid format and lowercase
- âœ“ Countries standardized to 9 values
- âœ“ Subscription plans: 5 consistent tiers
- âœ“ All booleans properly typed
- âœ“ Dates in ISO 8601 format
- âœ“ Numeric fields contain only numbers

âœ… **Transactions (100% Quality Score)**
- âœ“ No duplicate transaction_ids (1,570 unique)
- âœ“ All amounts numeric ($309,859 total)
- âœ“ Payment status: 4 standard values
- âœ“ Invoice numbers: "Inv-XXXX" format
- âœ“ Support tickets: "Tkt-XXXX" format
- âœ“ Date range validated (2020-2025)
- âœ“ All booleans properly typed

âš ï¸ **Known Issues**
- 77 transactions have invalid customer_ids (flagged for review)
- These transactions may need manual reconciliation

---

## ğŸ› Troubleshooting

### Common Issues

**Problem:** `ModuleNotFoundError: No module named 'pandas'`
```bash
Solution:
# Ensure virtual environment is activated (you should see (venv))
pip install -r requirements.txt
```

**Problem:** `FileNotFoundError: credentials.json not found`
```bash
Solution:
1. Download from Google Cloud Console
2. Rename to exactly "credentials.json"
3. Place in project root (same folder as README.md)
4. Verify with: ls credentials.json (Mac/Linux) or dir credentials.json (Windows)
```

**Problem:** `gspread.SpreadsheetNotFound: "SaaS Cleaned Data"`
```bash
Solution:
1. Create Google Sheet with exact name: "SaaS Cleaned Data"
2. Open credentials.json
3. Copy the "client_email" value
4. Share your Google Sheet with this email
5. Give "Editor" permissions
6. Uncheck "Notify people"
```

**Problem:** `ValueError: time data '...' does not match format`
```bash
Solution:
This is expected - invalid dates are automatically converted to NaT (Not a Time)
The script continues processing and flags these in the report
Check the cleaning log for how many dates were invalid
```

**Problem:** Jupyter kernel not found
```bash
Solution:
# Install kernel
python -m ipykernel install --user --name=cleaning-env

# In Jupyter: Kernel â†’ Change Kernel â†’ cleaning-env
```

---

## ğŸ“š Documentation

### Code Documentation

The main notebook (`Data_Cleaning_Complete.ipynb`) includes:
- Detailed inline comments
- Step-by-step explanations
- Before/after comparisons
- Progress tracking
- Error handling examples

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes
4. Test thoroughly
5. Commit: `git commit -m 'Add amazing feature'`
6. Push: `git push origin feature/amazing-feature`
7. Open a Pull Request

---


## ğŸ“„ License

MIT License

Copyright (c) 2025 Chinyere Obi

---

## ğŸ‘¤ Author

**Chinyere Obi**

Data Analyst | Python Developer | Data Quality Enthusiast

- ğŸŒ Portfolio: [mayreeobi.github.io](https://mayreeobi.github.io/)
- ğŸ’¼ LinkedIn: [chinyere-obi](https://www.linkedin.com/in/chinyere-obi)
- ğŸ™ GitHub: [@Mayreeobi](https://github.com/Mayreeobi)

---

## ğŸ™ Acknowledgments

- **Pandas Team** - For the incredible data manipulation library
- **Google** - For the Sheets API and comprehensive documentation
- **Python Community** - For inspiration and best practices
- **Stack Overflow** - For solving countless edge cases

---

## ğŸ’¼ Use Cases

Perfect for:

| User Type | Use Case |
|-----------|----------|
| ğŸ“Š **Data Analysts** | Automate repetitive cleaning tasks, focus on analysis |
| ğŸ’¼ **BI Teams** | Ensure dashboard data quality, real-time updates |
| ğŸ¢ **Small Businesses** | Clean CRM data without hiring data engineers |
| ğŸ“ **Students** | Learn production data engineering practices |
| ğŸš€ **Startups** | Scale data operations on limited budget |
| ğŸ‘¨â€ğŸ« **Educators** | Teach real-world data quality concepts |
| ğŸ”¬ **Researchers** | Prepare datasets for scientific analysis |

---

## ğŸ¯ Skills Demonstrated

This project showcases:

| Category | Skills |
|----------|--------|
| **Python** | Pandas, NumPy, OOP, functional programming, regex |
| **Data Engineering** | ETL pipelines, data quality, validation, transformation |
| **APIs** | REST APIs, OAuth2, service accounts, rate limiting |
| **Cloud** | Google Cloud Platform, Sheets API, Drive API |
| **Data Cleaning** | 10+ techniques, industry best practices, quality scoring |
| **Testing** | Data validation, quality checks, edge case handling |
| **Documentation** | Technical writing, markdown, inline comments |
| **Version Control** | Git, GitHub, branching, pull requests |

---

<div align="center">

### â­ If this project helped you, please star it!

**Made with â¤ï¸ and Python**

---

**Quick Links**
[ğŸ““ View Notebook](https://github.com/Mayreeobi/Automated-Data-Cleaning-Google-Sheets-Integration/blob/main/Data_Cleaning_Complete.ipynb) 

---

**ğŸ“š Resources**

[Pandas Documentation](https://pandas.pydata.org/docs/) â€¢ [Google Sheets API Guide](https://developers.google.com/sheets/api) 

---

**ğŸ“ˆ Project Metrics**

`4,500 records cleaned` â€¢ `1.77 seconds processing` â€¢ `100% quality score` â€¢ `99.994% time saved`

---

*Last Updated: November 14, 2025*

</div>
